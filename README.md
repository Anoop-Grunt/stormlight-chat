# ğŸŒ©ï¸ Stormlight Chat

An intelligent, persona-driven chat application built entirely on the **Cloudflare Edge**, combining real-time conversation, LLM-powered responses, and rich character-based interactions inspired by *The Stormlight Archive*.

---

## ğŸŒ Live Demo

Check out the live Stormlight Chat application deployed on **Cloudflare Pages**:

[https://stormlight-chat.pages.dev/](https://stormlight-chat.pages.dev/)

![Stormlight Chat Screenshot](./public/screenshot.png)

> The frontend is fully static, fast, and globally distributed, while the backend leverages Cloudflare Workers, Durable Objects, Workflows, and KV for real-time AI chat functionality.


## âš™ï¸ Architecture Overview

Stormlight Chat runs fully serverlessly on the Cloudflare developer platform, offering **edge-native AI chat** with minimal latency.

```mermaid
sequenceDiagram
    participant U as User (Browser)
    participant UI as Next.js UI / Cloudflare Pages
    participant CW as chat-room-do-worker
    participant WF as llm-workflow
    participant LC as chat-list-worker
    participant RC as chat-retrieve-worker
    participant KV as Chat KV Store
    participant AI as Cloudflare AI / LLaMA 3.1

    Note over UI,WF: Chat message flow (persona-based)

    U->>UI: Type message & select persona
    UI->>WF: POST /sendMessage {text, clientId, chatId, persona}
    WF->>KV: Retrieve or initialize chat context
    WF->>AI: Stream persona-aware LLaMA 3.1 tokens
    AI-->>WF: Stream incremental tokens
    WF->>CW: POST /send { message: token }
    CW-->>U: Stream token via SSE
    loop Until [DONE]
        AI-->>WF: Continue streaming tokens
        WF->>CW: Forward token to client
    end
    WF->>KV: Update chat history with assistant response
    WF->>CW: Send [DONE] signal
    CW-->>U: Complete SSE stream

    Note over UI,LC: Chat history listing
    UI->>LC: GET /listChats
    LC-->>UI: Return available chats

    Note over UI,RC: Chat retrieval
    UI->>RC: GET /retrieveChat?chatId={id}
    RC-->>UI: Return stored chat messages

```

### ğŸ§© Components

| Layer | Technology | Description |
|-------|------------|-------------|
| **Frontend** | [Next.js](https://nextjs.org) + [Jotai](https://jotai.org) | Interactive UI with reactive state for messages, personas, and session control. **Deployed as a static site on [Cloudflare Pages](https://pages.cloudflare.com/) for fast global delivery.** |
| **API Layer** | [Cloudflare Workers](https://developers.cloudflare.com/workers/) | REST endpoints, persona management, and LLM invocation logic. |
| **Streaming Engine** | [Durable Objects](https://developers.cloudflare.com/durable-objects/) | Maintains live HTTP/SSE streams for real-time chat updates. |
| **Background Jobs** | [Cloudflare Workflows](https://developers.cloudflare.com/workers/configuration/workflows/) | Executes LLM calls, token streaming, and state updates asynchronously. |
| **State Store** | [Cloudflare KV](https://developers.cloudflare.com/kv/) | Persists conversation history, persona info, and session metadata. |
| **AI Runtime** | [Cloudflare AI](https://developers.cloudflare.com/ai/) | Provides model inference for LLM-powered responses. |

---

## ğŸ§  Features

-  **Edge-native LLM chat** â€” ultra-low latency, globally distributed execution.  
-  **Real-time streaming** â€” token-level streaming from Durable Objects over SSE.  
- **Stormlight persona system** â€” chat with characters like Kaladin, Shallan, or Dalinar.  
-  **Persistent conversations** â€” chat state stored in Cloudflare KV.  
-  **Composable front-end** â€” powered by Jotai atoms and reusable UI primitives.  
- **Workflow orchestration** â€” background AI processes with resumable logic.  

---

## ğŸ§± Project Structure

# ğŸŒ©ï¸ Stormlight Chat

An intelligent, persona-driven chat application built entirely on the **Cloudflare Edge** â€” combining real-time conversation, LLM-powered responses, and rich character-based interactions inspired by *The Stormlight Archive*.

---

## âš™ï¸ Architecture Overview

Stormlight Chat runs fully serverlessly on the Cloudflare developer platform, offering **edge-native AI chat** with minimal latency.

### ğŸ§© Components

| Layer | Technology | Description |
|-------|------------|-------------|
| **Frontend** | [Next.js](https://nextjs.org) + [Jotai](https://jotai.org) | Interactive UI with reactive state for messages, personas, and session control. **Deployed as a static site on Cloudflare Pages** for fast global delivery. |
| **API Layer** | [Cloudflare Workers](https://developers.cloudflare.com/workers/) | REST endpoints, persona management, and LLM invocation logic. |
| **Streaming Engine** | [Durable Objects](https://developers.cloudflare.com/durable-objects/) | Maintains live HTTP/SSE streams for real-time chat updates. |
| **Background Jobs** | [Cloudflare Workflows](https://developers.cloudflare.com/workers/configuration/workflows/) | Executes LLM calls, token streaming, and state updates asynchronously. |
| **State Store** | [Cloudflare KV](https://developers.cloudflare.com/kv/) | Persists conversation history, persona info, and session metadata. |
| **AI Runtime** | [Cloudflare AI](https://developers.cloudflare.com/ai/) | Provides model inference for LLM-powered responses. |

--- 

## ğŸ§  LLM & Persona Integration

Stormlight Chat uses **LLaMA 3.1 8B Instruct** via Cloudflare AI to power its assistant responses. Each chat session is personalized using the selected **Stormlight Archive persona**, ensuring the conversation stays in-character.

### How it works

1. **Persona Context:**  
   Each chat session stores a system message that defines the persona, e.g.:

   > "You are an assistant based on Kaladin from the Stormlight Archive. Always make references to Kaladinâ€™s story in the books and never say something Kaladin wouldnâ€™t say."

2. **Appending User Messages:**  
   When a user sends a message, it is immediately added to the KV-stored chat history.

3. **Streaming AI Response:**  
   - The LLaMA model is called with the full conversation history (system + user messages).  
   - Responses are **streamed token-by-token** through a Durable Object to the client in real time.

4. **State Persistence:**  
   - Completed assistant responses are appended to the chat history in KV.  
   - This enables the workflow to maintain context across multiple messages and sessions.

### Benefits

- Maintains **in-character responses** for immersive roleplay.  
- Supports **long-running conversations** without losing context.  
- Provides **low-latency streaming** for smooth, real-time chat experiences.


---

## ğŸ§  Features

- âš¡ **Edge-native LLM chat** â€” ultra-low latency, globally distributed execution.  
- ğŸ§µ **Real-time streaming** â€” token-level streaming from Durable Objects over SSE.  
- ğŸ§â€â™‚ï¸ **Stormlight persona system** â€” chat with characters like Kaladin, Shallan, or Dalinar.  
- ğŸ’¾ **Persistent conversations** â€” chat state stored in Cloudflare KV.  
- ğŸ§© **Composable front-end** â€” powered by Jotai atoms and reusable UI primitives.  
- ğŸ”„ **Workflow orchestration** â€” background AI processes with resumable logic.  

---

## ğŸ§± Project Structure
```
stormlight-chat/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ globals.scss
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”œâ”€â”€ page.tsx
â”‚   â””â”€â”€ tailwind.css
â”‚
â”œâ”€â”€ atoms/                           #Jotai atoms
â”‚   â”œâ”€â”€ chatLog.ts
â”‚   â”œâ”€â”€ common.ts
â”‚   â””â”€â”€ debugLog.ts
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ stormlightchat/
â”‚   â”‚   â”œâ”€â”€ ChatCard.tsx
â”‚   â”‚   â””â”€â”€ DebugCard.tsx
â”‚   â””â”€â”€ ui/(shadcn components)
â”‚
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ utils.ts
â”‚
â”œâ”€â”€ public/
â”‚   â””â”€â”€ (static assets)
â”‚
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ audio-websocket/             # Worker handling bidirectional audio streaming
â”‚   â”‚   â”œâ”€â”€ worker.ts
â”‚   â”‚   â””â”€â”€ wrangler.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ fetch-chats/                 # Worker for retrieving all chats for a user/session
â”‚   â”‚   â”œâ”€â”€ worker.ts
â”‚   â”‚   â””â”€â”€ wrangler.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ model-worfklow/              # Cloudflare Workflow for orchestrating LLM calls + KV updates
â”‚   â”‚   â”œâ”€â”€ worker.ts
â”‚   â”‚   â””â”€â”€ wrangler.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieve-chat/               # Worker for fetching a single chatâ€™s history/state
â”‚   â”‚   â”œâ”€â”€ worker.ts
â”‚   â”‚   â””â”€â”€ wrangler.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ sse-do/                      # Durable Object managing SSE streaming sessions
â”‚   â”‚   â”œâ”€â”€ worker.ts
â”‚   â”‚   â””â”€â”€ wrangler.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ chat.d.ts                    # Shared type definitions
â”‚   â””â”€â”€ tsconfig.json                # Worker build configuration
â”‚
â”œâ”€â”€ commands.sh # wrangler deploy commands
â”œâ”€â”€ components.json
â”œâ”€â”€ next.config.js
â”œâ”€â”€ package.json
â”œâ”€â”€ pnpm-lock.yaml
â”œâ”€â”€ postcss.config.mjs
â”œâ”€â”€ tsconfig.json
```
